{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> <p>PostgreSQL schema and functions for Spatio-Temporal Asset Catalog (STAC)</p> </p> <p> </p> <p>Documentation: stac-utils.github.io/pgstac/</p> <p>Source Code: stac-utils/pgstac</p> <p>PgSTAC\u00a0is a set of SQL functions and schema to build highly performant databases for Spatio-Temporal Asset Catalogs (STAC). The project also provides\u00a0pypgstac (a Python module) to help with database migrations and document ingestion (collections and items).</p> <p>PgSTAC provides functionality for STAC Filters, CQL2 search, and utilities to help manage the indexing and partitioning of STAC Collections and Items.</p> <p>PgSTAC is used in production to scale to hundreds of millions of STAC items. PgSTAC implements core data models and functions to provide a STAC API from a PostgreSQL database. PgSTAC is entirely within the database and does not provide an HTTP-facing API. The\u00a0STAC FastAPI\u00a0PgSTAC backend and\u00a0Franklin\u00a0can be used to expose a PgSTAC catalog. Integrating PgSTAC with any other language with PostgreSQL drivers is also possible.</p> <p>PgSTAC Documentation:\u00a0stac-utils.github.io/pgstac/pgstac</p> <p>pyPgSTAC Documentation:\u00a0stac-utils.github.io/pgstac/pypgstac</p>"},{"location":"#project-structure","title":"Project structure","text":"<pre><code>/\n \u251c\u2500\u2500 src/pypgstac           - pyPgSTAC python module\n \u251c\u2500\u2500 src/pypgstac/tests/    - pyPgSTAC tests\n \u251c\u2500\u2500 scripts/               - scripts to set up the environment, create migrations, and run tests\n \u251c\u2500\u2500 src/pgstac/sql/        - PgSTAC SQL code\n \u251c\u2500\u2500 src/pgstac/migrations/ - Migrations for incremental upgrades\n \u2514\u2500\u2500 src/pgstac/tests/      - test suite\n</code></pre>"},{"location":"#contribution-development","title":"Contribution &amp; Development","text":"<p>See CONTRIBUTING.md</p>"},{"location":"#license","title":"License","text":"<p>See LICENSE</p>"},{"location":"#authors","title":"Authors","text":"<p>See contributors for a listing of individual contributors.</p>"},{"location":"#changes","title":"Changes","text":"<p>See CHANGELOG.md.</p>"},{"location":"contributing/","title":"Development - Contributing","text":"<p>PgSTAC uses a dockerized development environment. However, it still needs a local install of pypgstac to allow an editable install inside the docker container. This is installed automatically if you have set up a virtual environment for the project. Otherwise you'll need to install a local copy yourself by running <code>scripts/install</code>.</p> <p>To build the docker images and set up the test database, use:</p> <pre><code>scripts/setup\n</code></pre> <p>To bring up the development database: <pre><code>scripts/server\n</code></pre></p> <p>To run tests, use: <pre><code>scripts/test\n</code></pre></p> <p>To rebuild docker images: <pre><code>scripts/update\n</code></pre></p> <p>To drop into a console, use <pre><code>scripts/console\n</code></pre></p> <p>To drop into a psql console on the database container, use: <pre><code>scripts/console --db\n</code></pre></p> <p>To run migrations on the development database, use <pre><code>scripts/migrate\n</code></pre></p> <p>To stage code and configurations and create template migrations for a version release, use <pre><code>scripts/stageversion [version]\n</code></pre></p> <p>Examples: <pre><code>scripts/stageversion 0.2.8\n</code></pre></p> <p>This will create a base migration for the new version and will create incremental migrations between any existing base migrations. The incremental migrations that are automatically generated by this script will have the extension \".staged\" on the file. You must manually review (and make any modifications necessary) this file and remove the \".staged\" extension to enable the migration.</p>"},{"location":"contributing/#making-changes-to-sql","title":"Making Changes to SQL","text":"<p>All changes to SQL should only be made in the <code>/src/pgstac/sql</code> directory. SQL Files will be run in alphabetical order.</p>"},{"location":"contributing/#adding-tests","title":"Adding Tests","text":"<p>There are three different types of tests within the project: (1) pgTap tests, (2) basic SQL tests, and (3) PyPgSTAC tests.</p> <p>PgSTAC tests can be written using PGTap or basic SQL output comparisons. Additional testing is available using PyTest in the PyPgSTAC module. Tests can be run using the <code>scripts/test</code> command.</p> <p>PGTap tests can be written using PGTap syntax. Tests should be added to the <code>/src/pgstac/tests/pgtap</code> directory. Any new SQL files added to this directory must be added to <code>/src/pgstac/tests/pgtap.sql</code>.</p> <p>The Basic SQL tests will run any file ending in '.sql' in the <code>/src/pgstac/tests/basic</code> directory and will compare the exact results to the corresponding '.sql.out' file.</p> <p>PyPgSTAC tests are pytest tests, and they are located in <code>/src/pypgstac/tests</code></p> <p>All tests can be found in tests/pgtap.sql and are run using <code>scripts/test</code>.</p> <p>Individual tests can be run with any combination of the following flags <code>--formatting --basicsql --pgtap --migrations --pypgstac</code>. If pre-commit is installed, tests will be run on commit based on which files have changed.</p>"},{"location":"contributing/#to-make-a-pr","title":"To make a PR","text":"<p>1) Make any changes. 2) Make sure there are tests if appropriate. 3) Update Changelog using \"### Unreleased\" as the version. 4) Make any changes necessary to the docs. 5) Ensure all tests pass (pre-commit will take care of this if installed and the tests will also run on CI) 6) Create PR against the \"main\" branch.</p>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>1) Run \"scripts/stageversion VERSION\" (where version is the next version using semantic versioning ie 0.7.0 2) Check the incremental migration created in the /src/pgstac/migrations file with the .staged extension to make sure that the generated SQL looks appropriate. 3) Run the tests against the incremental migrations \"scripts/test --migrations\" 4) Move any \"Unreleased\" changes in the CHANGELOG.md to the new version. 5) Open a PR for the version change. 6) Once the PR has been merged, start the release process. 7) Create a git tag <code>git tag v0.2.8</code> using new version number 8) Push the git tag <code>git push origin v0.2.8</code> 9) The CI process will push pypgstac to PyPi, create a docker image on ghcr.io, and create a release on github.</p>"},{"location":"contributing/#get-involved","title":"Get Involved","text":"<p>Issues and pull requests are more than welcome: github.com/stac-utils/pgstac/issues</p>"},{"location":"contributing/#a-note-on-hydration-and-dehydration","title":"A Note on Hydration and Dehydration","text":"<p>Dehydration refers to stripping redundant attributes of STAC items when storing them within the database. For many collections, dehydration saves a significant amount of memory.</p> <p>Rehydration is the process of adding the stripped attributes back to the STAC items, such as during the export of an STAC collection or the response to a search query.</p> <p>PgSTAC, a versatile tool, is designed to seamlessly integrate with PyPgSTAC or alternative backends. This flexibility allows for direct calls for both rehydration and dehydration, giving developers and technical users a sense of control over the process.</p> <p>Hydration and dehydration are de-facto settings that users can not opt out of. In the future, we may provide a configuration for use cases where the size benefits do not justify the added complexity.</p>"},{"location":"item_size_analysis/","title":"impacts of STAC item footprint size on dynamic tiling query performance","text":"Out[2]: tile width (degrees) # items 0 0.5 10000 1 1.0 2500 2 2.0 625 3 4.0 169 4 6.0 81 5 8.0 49 6 10.0 25 <p>The number of items is inversely proportional to the square of the tile width which means that small changes in tile size can have a large impact on the eventual number of items in your catalog!</p> <p>This map shows the spatial arrangement of the items for a range of tile sizes:</p> Out[3]: Make this Notebook Trusted to load map: File -&gt; Trust Notebook <pre>&lt;Axes: xlabel='zoom level', ylabel='item tile width'&gt;</pre> <p>Without details about the resource configuration for a specific <code>pgstac</code> deployment it is hard to say which zoom level becomes inoperable for a given tile size, but queries that take &gt;0.5 seconds in this test would probably yield poor results in a deployed context.</p>"},{"location":"item_size_analysis/#impacts-of-stac-item-footprint-size-on-dynamic-tiling-query-performance","title":"impacts of STAC item footprint size on dynamic tiling query performance\u00b6","text":"<p>TL;DR: If you have any control over the geographic footprint of the assets that you are cataloging with <code>pgstac</code> and you want to serve visualizations with a dynamic tiling application, try to maximize the size of the assets!</p> <p>Dynamic tiling applications like <code>titiler-pgstac</code> send many queries to a <code>pgstac</code> database and clients are very sensitive to performance so it is worth considering a few basic ideas when building collections and items that may be used in this way.</p> <p><code>pgstac</code>'s query functions perform relatively expensive spatial intersection operations so the fewer items there are in a collection x datetime partition, the faster the query will be. This is not a <code>pgstac</code>-specific problem (any application that needs to perform spatial intersections will take longer as the number of calculations increases), but it is worth demonstrating the influence of these factors in the dynamic tiling context.</p>"},{"location":"item_size_analysis/#scenario","title":"Scenario\u00b6","text":"<p>Imagine you have a continental-scale dataset of gridded data that will be stored as cloud-optimized geotiffs (COGs) and you get to decide how the individual files will be spatially arranged and cataloged in a <code>pgstac</code> database. You could make items as small as 0.5 degree squares or as large as 10 degree squares. In this case the assets will be non-overlapping rectangular grids.</p> <p>The assets will be publicly accessible, so smaller file sizes might be useful for some applications/users, but since the data will be stored as COGs and we also want to be able to serve raster tile visualizations in a web map with <code>titiler-pgstac</code>, smaller file sizes are not very important. However, the processing pipleline that generates the assets might have some resource constraints that push you to choose a smaller tile size.</p> <p>Consider the following options for tile sizes:</p>"},{"location":"item_size_analysis/#performance-comparison","title":"Performance comparison\u00b6","text":"<p>To simulate the performance of queries made by a dynamic tiling application we have prepared a benchmarking procedure that uses the <code>pgstac</code> function <code>xyzsearch</code> to run an item query for an XYZ tile. By iterating over many combinations of tile sizes and zoom levels we can examine the response time with respect to item footprint size and tile zoom level.</p> <p>This figure shows average response time for <code>xyzsearch</code> to return a complete set of results for each zoom level for the range of item tile widths:</p>"},{"location":"pgstac/","title":"PgSTAC","text":"<p>PGDatabase Schema and Functions for Storing and Accessing STAC collections and items in PostgreSQL</p> <p>STAC Client that uses PgSTAC available in STAC-FastAPI</p> <p>PgSTAC requires Postgresql&gt;=13, PostGIS&gt;=3 and btree_gist. Best performance will be had using PostGIS&gt;=3.1.</p>"},{"location":"pgstac/#pgstac-settings","title":"PgSTAC Settings","text":"<p>PgSTAC installs everything into the pgstac schema in the database. This schema must be in the search_path in the postgresql session while using pgstac.</p>"},{"location":"pgstac/#pgstac-users","title":"PgSTAC Users","text":"<p>The <code>pgstac_admin</code> role is the owner of all the objects within pgstac and should be used when running things such as migrations.</p> <p>The <code>pgstac_ingest</code> role has read/write privileges on all tables and should be used for data ingest or if using the transactions extension with stac-fastapi-pgstac.</p> <p>The <code>pgstac_read</code> role has read only access to the items and collections, but will still be able to write to the logging tables.</p> <p>You can use the roles either directly and adding a password to them or by granting them to a role you are already using.</p> <p>To use directly: <pre><code>ALTER ROLE pgstac_read LOGIN PASSWORD '&lt;password&gt;';\n</code></pre></p> <p>To grant pgstac permissions to a current postgresql user: <pre><code>GRANT pgstac_read TO &lt;user&gt;;\n</code></pre></p>"},{"location":"pgstac/#pgstac-search-path","title":"PgSTAC Search Path","text":"<p>The search_path can be set at the database level or role level or by setting within the current session. The search_path is already set if you are directly using one of the pgstac users. If you are not logging in directly as one of the pgstac users, you will need to set the search_path by adding it to the search_path of the user you are using: <pre><code>ALTER ROLE &lt;user&gt; SET SEARCH_PATH TO pgstac, public;\n</code></pre> setting the search_path on the database: <pre><code>ALTER DATABASE &lt;database&gt; set search_path to pgstac, public;\n</code></pre></p> <p>In psycopg the search_path can be set by passing it as a configuration when creating your connection: <pre><code>kwargs={\n    \"options\": \"-c search_path=pgstac,public\"\n}\n</code></pre></p>"},{"location":"pgstac/#pgstac-settings-variables","title":"PgSTAC Settings Variables","text":"<p>There are additional variables that control the settings used for calculating and displaying context (total row count) for a search, as well as a variable to set the filter language (cql-json or cql2-json). The context is \"off\" by default, and the default filter language is set to \"cql2-json\".</p> <p>Variables can be set either by passing them in via the connection options using your connection library, setting them in the pgstac_settings table or by setting them on the Role that is used to log in to the database.</p> <p>Turning \"context\" on can be very expensive on larger databases. Much of what PgSTAC does is to optimize the search of items sorted by time where only fewer than 10,000 records are returned at a time. It does this by searching for the data in chunks and is able to \"short circuit\" and return as soon as it has the number of records requested. Calculating the context (the total count for a query) requires a scan of all records that match the query parameters and can take a very long time. Setting \"context\" to auto will use database statistics to estimate the number of rows much more quickly, but for some queries, the estimate may be quite a bit off.</p> <p>Example for updating the pgstac_settings table with a new value: <pre><code>INSERT INTO pgstac_settings (name, value)\nVALUES\n    ('default-filter-lang', 'cql-json'),\n    ('context', 'on')\n\nON CONFLICT ON CONSTRAINT pgstac_settings_pkey DO UPDATE SET value = excluded.value;\n</code></pre></p> <p>Alternatively, update the role: <pre><code>ALTER ROLE &lt;username&gt; SET SEARCH_PATH to pgstac, public;\nALTER ROLE &lt;username&gt; SET pgstac.context TO &lt;'on','off','auto'&gt;;\nALTER ROLE &lt;username&gt; SET pgstac.context_estimated_count TO '&lt;number of estimated rows when in auto mode that when an estimated count is less than will trigger a full count&gt;';\nALTER ROLE &lt;username&gt; SET pgstac.context_estimated_cost TO '&lt;estimated query cost from explain when in auto mode that when an estimated cost is less than will trigger a full count&gt;';\nALTER ROLE &lt;username&gt; SET pgstac.context_stats_ttl TO '&lt;an interval string ie \"1 day\" after which pgstac search will force recalculation of it's estimates&gt;&gt;';\n</code></pre></p> <p>The check_pgstac_settings function can be used to check what pgstac settings are being used and to check recommendations for system settings. It takes a single parameter which should be the amount of memory available on the database system. <pre><code>SELECT check_pgstac_settings('16GB');\n</code></pre></p>"},{"location":"pgstac/#read-only-mode","title":"Read Only Mode","text":"<p>The pgstac.readonly setting can be used when using pgstac with a read replica. Note that when pgstac.readonly is set to TRUE that pgstac is unable to use a cache for calculating the total count for context which can make use of the context extension very expensive (see notes above). In readonly mode, pgstac is also unable to register the hash that is used to store queries that can be used with geometry_search (used by titiler-pgstac). A registered hash will still be readable, but new hashes cannot be created on the read only replica, they must be registered on the main database.</p>"},{"location":"pgstac/#runtime-configurations","title":"Runtime Configurations","text":"<p>Runtime configuration of variables can be made with search by passing in configuration in the search json \"conf\" item.</p> <p>Runtime configuration is available for context, context_estimated_count, context_estimated_cost, context_stats_ttl, and nohydrate.</p> <p>The nohydrate conf item returns an unhydrated item bypassing the CPU intensive step of rehydrating data with data from the collection metadata. When using the nohydrate conf, the only fields that are respected in the fields extension are geometry and bbox. <pre><code>SELECT search('{\"conf\":{\"nohydrate\"=true}}');\n</code></pre></p>"},{"location":"pgstac/#pgstac-partitioning","title":"PgSTAC Partitioning","text":"<p>By default PgSTAC partitions data by collection (note: this is a change starting with version 0.5.0). Each collection can further be partitioned by either year or month. Partitioning must be set up prior to loading any data! Partitioning can be configured by setting the partition_trunc flag on a collection in the database. <pre><code>UPDATE collections set partition_trunc='month' WHERE id='&lt;collection id&gt;';\n</code></pre></p> <p>In general, you should aim to keep each partition less than a few hundred thousand rows. Further partitioning (ie setting everything to 'month' when not needed to keep the partitions below a few hundred thousand rows) can be detrimental.</p>"},{"location":"pgstac/#pgstac-indexes-queryables","title":"PgSTAC Indexes / Queryables","text":"<p>By default, PgSTAC includes indexes on the id, datetime, collection, and geometry. Further indexing can be added for additional properties globally or only on particular collections by modifications to the queryables table.</p> <p>The <code>queryables</code> table controls the indexes that PgSTAC will build as well as the metadata that is returned from a STAC Queryables endpoint.</p> Column Description Type Example <code>id</code> The id of the queryable bigint[pk] - <code>name</code> The name of the property text <code>eo:cloud_cover</code> <code>collection_ids</code> The collection ids that this queryable applies to text[] <code>{sentinel-2-l2a,landsat-c2-l2,aster-l1t}</code> or <code>NULL</code> <code>definition</code> The queryable definition of the property jsonb <code>{\"title\": \"Cloud Cover\", \"type\": \"number\", \"minimum\": 0, \"maximum\": 100}</code> <code>property_wrapper</code> The wrapper function to use to convert the property to a searchable type text One of <code>to_int</code>, <code>to_float</code>, <code>to_tstz</code>, <code>to_text</code> or <code>NULL</code> <code>property_index_type</code> The index type to use for the property text <code>BTREE</code>, <code>NULL</code> or other valid PostgreSQL index type <p>Each record in the queryables table references a single property but can apply to any number of collections. If the <code>collection_ids</code> field is left as NULL, then that queryable will apply to all collections. There are constraints that allow only a single queryable record to be active per collection. If there is a queryable already set for a property field with collection_ids set to NULL, you will not be able to create a separate queryable entry that applies to that property with a specific collection as pgstac would not then be able to determine which queryable entry to use.</p> <p>By default, any property may be used in filter expressions. If you wish to restrict it and only allow the queryables, you should either set the additional_properties setting variable to False or make the corresponding adjustment in the pgstac_settings table.</p>"},{"location":"pgstac/#queryable-metadata","title":"Queryable Metadata","text":"<p>When used with stac-fastapi, the metadata returned in the queryables endpoint is determined using the definition field on the <code>queryables</code> table. This is a jsonb field that will be returned as-is in the queryables response. The full queryable response for a collection will be determined by all the <code>queryables</code> records that have a match in <code>collection_ids</code> or have a NULL <code>collection_ids</code>.</p> <p>If two or more collections in your catalog share a property name, but have different definitions (e.g., <code>platform</code> with different enum values), be sure to repeat the property for each collection id, each with a unique <code>definition</code>.</p> <p>There is a utility SQL function that can be used to help populate the <code>queryables</code> table by looking at a sample of data for each collection. This utility can also look to the json schema for STAC extensions defined in the <code>stac_extensions</code> table.</p> <p>The <code>stac_extensions</code> table contains a <code>url</code> field and a <code>content</code> field for each extension that should be introspected to compare for fields. This can either be filled in manually or by using the <code>pypgstac loadextensions</code> command included with pypgstac. This command will look at the <code>stac_extensions</code> attribute in all collections to populate the <code>stac_extensions</code> table, fetching the json content of each extension. If any urls were added manually to the stac_extensions table, it will also populate any records where the content is NULL.</p> <p>Once the <code>stac_extensions</code> table has been filled in, you can run the <code>missing_queryables</code> function either for a single collection:</p> <pre><code>SELECT * FROM missing_queryables('mycollection', 5);\n</code></pre> <p>or for all collections:</p> <pre><code>SELECT * FROM missing_queryables(5);\n</code></pre> <p>The numeric argument is the approximate percent of items that should be sampled to look for fields to include. This function will look for fields in the properties of items that do not already exist in the queryables table for each collection. It will then look to see if there is a field in any definition in the stac_extensions table to populate the definition for the queryable. If no definition was found, it will use the data type of the values for that field in the sample of items to fill in a generic definition with just the field type.</p> <p>In order to populate the queryables table, you can then run the following query. Note we're casting the collection id to a text array:</p> <pre><code>INSERT INTO queryables (collection_ids, name, definition, property_wrapper)\n    SELECT array[collection]::text[] as collection_ids, name, definition, property_wrapper\n        FROM missing_queryables('mycollection', 5)\n</code></pre> <p>If you run into conflicts due to the unique constraints on collection/name, you may need to create a temp table, make any changes to remove the conflicts, and then INSERT.</p> <pre><code>CREATE TEMP TABLE draft_queryables AS SELECT * FROM missing_queryables(5);\n</code></pre> <p>Make any edits to that table or the existing queryables, then:</p> <pre><code>INSERT INTO queryables (collection_ids, name, definition, property_wrapper) SELECT * FROM draft_queryables;\n</code></pre>"},{"location":"pgstac/#indexing","title":"Indexing","text":"<p>The <code>queryables</code> table is also used to specify which item <code>properties</code> attributes to add indexes on.</p> <p>To add a new global index across all collection partitions:</p> <pre><code>INSERT INTO pgstac.queryables (name, property_wrapper, property_index_type)\nVALUES (&lt;property name&gt;, &lt;property wrapper&gt;, &lt;index type&gt;);\n</code></pre> <p>Property wrapper should be one of <code>to_int</code>, <code>to_float</code>, <code>to_tstz</code>, or <code>to_text</code>. The index type should almost always be <code>BTREE</code>, but can be any PostgreSQL index type valid for the data type.</p> <p>More indexes is not necessarily better. You should only index the primary fields that are actively being used to search. Adding too many indexes can be very detrimental to performance and ingest speed. If your primary use case is delivering items sorted by datetime and you do not use the context extension, you likely will not need any further indexes.</p> <p>Leave <code>property_index_type</code> set to NULL if you do not want an index set for a property.</p>"},{"location":"pgstac/#maintenance-procedures","title":"Maintenance Procedures","text":"<p>These are procedures that should be run periodically to make sure that statistics and constraints are kept up-to-date and validated. These can be made to run regularly using the pg_cron extension if available. <pre><code>SELECT cron.schedule('0 * * * *', 'CALL validate_constraints();');\nSELECT cron.schedule('10, * * * *', 'CALL analyze_items();');\n</code></pre></p>"},{"location":"pgstac/#system-checks","title":"System Checks","text":""},{"location":"pgstac/#system-and-pgstac-settings","title":"System and pgSTAC Settings","text":"<p>PgSTAC includes a function for checking common Postgres settings. You must pass in the amount of total memory available to get appropriate memory settings. Do note that these suggestions are merely a good first pass and figuring out the ideal parameters requires further analysis of a system and logs. <pre><code>SELECT * FROM check_pgstac_settings('32GB');\n</code></pre></p>"},{"location":"pgstac/#unused-indexes","title":"Unused Indexes","text":"<p>Having too many indexes that do not get used can drastically hurt the performance of the database. The following query can be used to show indexes that are not getting used and should be considered for removal. <pre><code>SELECT\n    relname, indexrelname, idx_scan, last_idx_scan,idx_tup_read, idx_tup_fetch,\n    pg_relation_size(relid) as index_bytes,\n    pg_size_pretty(pg_relation_size(relid)) as index_size\nFROM pg_stat_user_indexes\nWHERE schemaname = 'pgstac'\nORDER BY idx_scan ASC\n;\n</code></pre></p>"},{"location":"pgstac/#notification-triggers","title":"Notification Triggers","text":"<p>You can add notification triggers alongside pgstac to get notified when items are inserted, updated, or deleted.</p> <p>Important: Do NOT create these in the <code>pgstac</code> schema as they could be removed in future migrations.</p>"},{"location":"pgstac/#example-notification-setup","title":"Example Notification Setup","text":"<p>Here's an example of how to set up notification triggers for item changes:</p> <pre><code>-- Create the notification function\nCREATE OR REPLACE FUNCTION notify_items_change_func()\nRETURNS TRIGGER AS $$\nDECLARE\n\nBEGIN\n    PERFORM pg_notify('pgstac_items_change'::text, json_build_object(\n            'operation', TG_OP,\n            'items', jsonb_agg(\n                jsonb_build_object(\n                    'collection', data.collection,\n                    'id', data.id\n                )\n            )\n        )::text\n        )\n        FROM data\n    ;\n    RETURN NULL;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create triggers for INSERT operations\nCREATE OR REPLACE TRIGGER notify_items_change_insert\n    AFTER INSERT ON pgstac.items\n    REFERENCING NEW TABLE AS data\n    FOR EACH STATEMENT EXECUTE FUNCTION notify_items_change_func()\n;\n\n-- Create triggers for UPDATE operations\nCREATE OR REPLACE TRIGGER notify_items_change_update\n    AFTER UPDATE ON pgstac.items\n    REFERENCING NEW TABLE AS data\n    FOR EACH STATEMENT EXECUTE FUNCTION notify_items_change_func()\n;\n\n-- Create triggers for DELETE operations\nCREATE OR REPLACE TRIGGER notify_items_change_delete\n    AFTER DELETE ON pgstac.items\n    REFERENCING OLD TABLE AS data\n    FOR EACH STATEMENT EXECUTE FUNCTION notify_items_change_func()\n;\n</code></pre>"},{"location":"pgstac/#usage","title":"Usage","text":"<p>Listen for notifications: <pre><code>LISTEN pgstac_items_change;\n</code></pre></p> <p>Payload structure:</p> <pre><code>{\n  \"operation\": \"INSERT\",\n  \"items\": [\n    {\n      \"collection\": \"sentinel-2-l2a\",\n      \"id\": \"item-1\"\n    },\n    {\n      \"collection\": \"sentinel-2-l2a\",\n      \"id\": \"item-2\"\n    }\n  ]\n}\n</code></pre>"},{"location":"pgstac/#customization","title":"Customization","text":"<p>You may modify the function to include additional metadata, filter by collection, or use different channels. Only trigger on the <code>items</code> table, not <code>items_staging</code>.</p>"},{"location":"pypgstac/","title":"pyPgSTAC","text":"<p>PgSTAC includes a Python utility for bulk data loading and managing migrations.</p> <p>pyPgSTAC is available on PyPI <pre><code>python -m pip install pypgstac\n</code></pre></p> <p>By default, pyPgSTAC does not install the <code>psycopg</code> dependency. If you want the database driver installed, use:</p> <pre><code>python -m pip install pypgstac[psycopg]\n</code></pre> <p>Or can be built locally <pre><code>git clone https://github.com/stac-utils/pgstac\ncd pgstac/pypgstac\npython -m pip install .\n</code></pre></p> <pre><code>pypgstac --help\nUsage: pypgstac [OPTIONS] COMMAND [ARGS]...\n\nOptions:\n  --install-completion  Install completion for the current shell.\n  --show-completion     Show completion for the current shell, to copy it or\n                        customize the installation.\n\n  --help                Show this message and exit.\n\nCommands:\n  initversion  Get initial version.\n  load         Load STAC data into a pgstac database.\n  migrate      Migrate a pgstac database.\n  pgready      Wait for a pgstac database to accept connections.\n  version      Get version from a pgstac database.\n</code></pre> <p>pyPgSTAC will get the database connection settings from the standard PG environment variables:</p> <ul> <li>PGHOST=0.0.0.0</li> <li>PGPORT=5432</li> <li>PGUSER=username</li> <li>PGDATABASE=postgis</li> <li>PGPASSWORD=asupersecretpassword</li> </ul> <p>It can also take a DSN database url \"postgresql://...\" via the --dsn flag.</p>"},{"location":"pypgstac/#migrations","title":"Migrations","text":"<p>pyPgSTAC has a utility to help apply migrations to an existing PgSTAC instance to bring it up to date.</p> <p>There are two types of migrations:</p> <ul> <li>Base migrations install PgSTAC into a database with no current PgSTAC installation. These migrations follow the file pattern <code>\"pgstac.[version].sql\"</code></li> <li>Incremental migrations are used to move PgSTAC from one version to the next. These migrations follow the file pattern <code>\"pgstac.[version].[fromversion].sql\"</code></li> </ul> <p>Migrations are stored in <code>pypgstac/pypgstac/migrations</code> and are distributed with the pyPgSTAC package.</p>"},{"location":"pypgstac/#running-migrations","title":"Running Migrations","text":"<p>pyPgSTAC has a utility for checking the version of an existing PgSTAC database and applying the appropriate migrations in the correct order. It can also be used to setup a database from scratch.</p> <p>To create an initial PgSTAC database or bring an existing one up to date, check you have the pypgstac version installed you want to migrate to and run: <pre><code>pypgstac migrate\n</code></pre></p>"},{"location":"pypgstac/#bootstrapping-an-empty-database","title":"Bootstrapping an Empty Database","text":"<p>When starting with an empty database, you have two options for initializing PgSTAC:</p>"},{"location":"pypgstac/#option-1-execute-as-power-user","title":"Option 1: Execute as Power User","text":"<p>This approach uses a database user with administrative privileges (such as 'postgres') to run the migration, which will automatically create all necessary extensions and roles:</p> <pre><code># Set environment variables for database connection\nexport PGHOST=localhost\nexport PGPORT=5432\nexport PGDATABASE=yourdatabase\nexport PGUSER=postgres  # A user with admin privileges\nexport PGPASSWORD=yourpassword\n\n# Run the migration\npypgstac migrate\n</code></pre> <p>The migration process will automatically: - Create required extensions (postgis, btree_gist, unaccent) - Create necessary roles (pgstac_admin, pgstac_read, pgstac_ingest) - Set up the pgstac schema and tables</p> <p>In production environments, you should assign these roles to your application database user rather than continuing to use the postgres user:</p> <pre><code>-- Grant appropriate roles to your application user\nGRANT pgstac_read TO your_app_user;\nGRANT pgstac_ingest TO your_app_user;\nGRANT pgstac_admin TO your_app_user;\n\n-- Set the search path for your application user\nALTER USER your_app_user SET search_path TO pgstac, public;\n</code></pre>"},{"location":"pypgstac/#option-2-create-user-with-initial-grants","title":"Option 2: Create User with Initial Grants","text":"<p>If you don't have administrative privileges or prefer more control over the setup process, you can manually prepare the database before running migrations.</p> <p>Connect to your database as an administrator and execute:</p> <pre><code>\\c [database]\n\n-- Create required extensions\nCREATE EXTENSION IF NOT EXISTS postgis;\nCREATE EXTENSION IF NOT EXISTS btree_gist;\nCREATE EXTENSION IF NOT EXISTS unaccent;\n\n-- Create required roles\nCREATE ROLE pgstac_admin;\nCREATE ROLE pgstac_read;\nCREATE ROLE pgstac_ingest;\n\n-- Grant appropriate permissions\nALTER DATABASE [database] OWNER TO [user];\nALTER USER [user] SET search_path TO pgstac, public;\nALTER DATABASE [database] set search_path to pgstac, public;\nGRANT CONNECT ON DATABASE [database] TO [user];\nGRANT ALL PRIVILEGES ON TABLES TO [user];\nGRANT ALL PRIVILEGES ON SEQUENCES TO [user];\nGRANT pgstac_read TO [user] WITH ADMIN OPTION;\nGRANT pgstac_ingest TO [user] WITH ADMIN OPTION;\nGRANT pgstac_admin TO [user] WITH ADMIN OPTION;\n</code></pre> <p>Then run the migration as your non-admin user:</p> <pre><code># Set environment variables for database connection\nexport PGHOST=localhost\nexport PGPORT=5432\nexport PGDATABASE=yourdatabase\nexport PGUSER=[user]  # Your non-admin user\nexport PGPASSWORD=yourpassword\n\n# Run the migration\npypgstac migrate\n</code></pre>"},{"location":"pypgstac/#verifying-migration","title":"Verifying Migration","text":"<p>To verify that PgSTAC was installed correctly:</p> <pre><code># Check the PgSTAC version\npypgstac version\n</code></pre>"},{"location":"pypgstac/#bulk-data-loading","title":"Bulk Data Loading","text":"<p>A python utility is included which allows to load data from any source openable by smart-open using python in a memory efficient streaming manner using PostgreSQL copy. There are options for collections and items and can be used either as a command line or a library.</p> <p>To load an ndjson of items directly using copy (will fail on any duplicate ids but is the fastest option to load new data you know will not conflict) <pre><code>pypgstac load items\n</code></pre></p> <p>To load skipping any records that conflict with existing data <pre><code>pypgstac load items --method insert_ignore\n</code></pre></p> <p>To upsert any records, adding anything new and replacing anything with the same id <pre><code>pypgstac load items --method upsert\n</code></pre></p>"},{"location":"pypgstac/#loading-queryables","title":"Loading Queryables","text":"<p>Queryables are a mechanism that allows clients to discover what terms are available for use when writing filter expressions in a STAC API. The Filter Extension enables clients to filter collections and items based on their properties using the Common Query Language (CQL2).</p> <p>To load queryables from a JSON file:</p> <pre><code>pypgstac load_queryables queryables.json\n</code></pre> <p>To load queryables for specific collections:</p> <pre><code>pypgstac load_queryables queryables.json --collection_ids [collection1,collection2]\n</code></pre> <p>To load queryables and delete properties not present in the file:</p> <pre><code>pypgstac load_queryables queryables.json --delete_missing\n</code></pre> <p>To load queryables and create indexes only for specific fields:</p> <pre><code>pypgstac load_queryables queryables.json --index_fields [field1,field2]\n</code></pre> <p>By default, no indexes are created when loading queryables. Using the <code>--index_fields</code> parameter allows you to selectively create indexes only for fields that require them. Creating too many indexes can degrade database performance, especially for write operations, so it's recommended to only index fields that are frequently used in queries.</p> <p>When using <code>--delete_missing</code> with specific collections, only properties for those collections will be deleted:</p> <pre><code>pypgstac load_queryables queryables.json --collection_ids [collection1,collection2] --delete_missing\n</code></pre> <p>You can combine all parameters as needed:</p> <pre><code>pypgstac load_queryables queryables.json --collection_ids [collection1,collection2] --delete_missing --index_fields [field1,field2]\n</code></pre> <p>The JSON file should follow the queryables schema as described in the STAC API - Filter Extension. Here's an example:</p> <pre><code>{\n  \"$schema\": \"https://json-schema.org/draft/2019-09/schema\",\n  \"$id\": \"https://example.com/stac/queryables\",\n  \"type\": \"object\",\n  \"title\": \"Queryables for Example STAC API\",\n  \"description\": \"Queryable names for the Example STAC API\",\n  \"properties\": {\n    \"id\": {\n      \"description\": \"Item identifier\",\n      \"type\": \"string\"\n    },\n    \"datetime\": {\n      \"description\": \"Datetime\",\n      \"type\": \"string\",\n      \"format\": \"date-time\"\n    },\n    \"eo:cloud_cover\": {\n      \"description\": \"Cloud cover percentage\",\n      \"type\": \"number\",\n      \"minimum\": 0,\n      \"maximum\": 100\n    }\n  },\n  \"additionalProperties\": true\n}\n</code></pre> <p>The command will extract the properties from the JSON file and create queryables in the database. It will also determine the appropriate property wrapper based on the type of each property and create the necessary indexes.</p>"},{"location":"pypgstac/#automated-collection-extent-updates","title":"Automated Collection Extent Updates","text":"<p>By setting <code>pgstac.update_collection_extent</code> to <code>true</code>, a trigger is enabled to automatically adjust the spatial and temporal extents in collections when new items are ingested. This feature, while helpful, may increase overhead within data load transactions. To alleviate performance impact, combining this setting with <code>pgstac.use_queue</code> is beneficial. This approach necessitates a separate process, such as a scheduled task via the <code>pg_cron</code> extension, to periodically invoke <code>CALL run_queued_queries();</code>. Such asynchronous processing ensures efficient transactional performance and updated collection extents.</p> <p>Note: The <code>pg_cron</code> extension must be properly installed and configured to manage the scheduling of the <code>run_queued_queries()</code> function.</p>"},{"location":"release-notes/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog and this project adheres to Semantic Versioning.</p>"},{"location":"release-notes/#v098","title":"v0.9.8","text":""},{"location":"release-notes/#fixed","title":"Fixed","text":"<ul> <li>Allow array as q parameter for full text search</li> </ul>"},{"location":"release-notes/#v097","title":"v0.9.7","text":""},{"location":"release-notes/#fixed_1","title":"Fixed","text":"<ul> <li>Fix bad handling of leading \u00b1 terms in free-text search</li> <li>Use consistent tsquery config in free-text search</li> </ul>"},{"location":"release-notes/#v096","title":"v0.9.6","text":""},{"location":"release-notes/#added","title":"Added","text":"<ul> <li>Add <code>load_queryables</code> function to pypgstac for loading queryables from a JSON file</li> <li>Add support for specifying collection IDs when loading queryables</li> </ul>"},{"location":"release-notes/#fixed_2","title":"Fixed","text":"<ul> <li>Added missing 0.8.6-0.9.0 migration script</li> </ul>"},{"location":"release-notes/#v095","title":"v0.9.5","text":""},{"location":"release-notes/#changed","title":"Changed","text":"<ul> <li>Pin to <code>plpygic&gt;=0.5.0</code> and use <code>geom.ewkb</code> instead of <code>geom.wkt</code> when formatting items in <code>Loader.format_item</code>. Fixes (#357)</li> </ul>"},{"location":"release-notes/#v094","title":"v0.9.4","text":""},{"location":"release-notes/#changed_1","title":"Changed","text":"<ul> <li>Relax pypgstac dependencies</li> </ul>"},{"location":"release-notes/#v093","title":"v0.9.3","text":""},{"location":"release-notes/#fixed_3","title":"Fixed","text":"<ul> <li>Fix CI issue with tests not running</li> <li>Fix for issue with nulls in title or keywords for free text search</li> </ul>"},{"location":"release-notes/#changed_2","title":"Changed","text":"<ul> <li>Replace hardcoded org name in CI</li> </ul>"},{"location":"release-notes/#v092","title":"v0.9.2","text":""},{"location":"release-notes/#added_1","title":"Added","text":"<ul> <li>Add limited support for free-text search in the search functions. (Fixes #293)</li> <li>the <code>q</code> parameter is converted from the     OGC API - Features syntax into a <code>tsquery</code>     statement which is used to compare to the description, title, and keywords fields in items or collection_search</li> <li>the text search is un-indexed and will be very slow for item-level searches!</li> <li>Add support for Postgres 17</li> <li>Support for adding data to the private field using the pypgstac loader</li> </ul>"},{"location":"release-notes/#fixed_4","title":"Fixed","text":"<ul> <li>Add <code>open=True</code> in <code>psycopg.ConnectionPool</code> to avoid future behavior change</li> <li>Switch from postgres <code>server_version</code> to <code>server_version_num</code> to get PG version (Fixes #300)</li> <li>Allow read-only replicas work even when the context extension is enabled (Fixes #300)</li> <li>Consistently ensure use of instantiated postgres fields when addressing with 'properties.' prefix</li> </ul>"},{"location":"release-notes/#changed_3","title":"Changed","text":"<ul> <li>Move rust hydration to a separate repo</li> </ul>"},{"location":"release-notes/#v091","title":"v0.9.1","text":""},{"location":"release-notes/#fixed_5","title":"Fixed","text":"<ul> <li>Fixed double nested extent when using trigger based update collection extent. (Fixes #274)</li> <li>Fix time formatting (Fixes #275)</li> <li>Relaxes smart-open dependency check (Fixes #273)</li> <li>Switch to uv for docker image</li> </ul>"},{"location":"release-notes/#v090","title":"v0.9.0","text":""},{"location":"release-notes/#breaking-changes","title":"Breaking Changes","text":"<ul> <li>Context Extension has been deprecated. Context is now reported using OGC Features compliant numberMatched and numberReturned</li> <li>Paging return from search using prev/next properties has been deprecated. Paging is now available in the spec compliant Links</li> </ul>"},{"location":"release-notes/#added_2","title":"Added","text":"<ul> <li>Add support for Casei and Accenti (Fixes #237). (Also, requires the addition of the unaccent extension)</li> <li>Add numberReturned and numberMatched fields for ItemCollection. BREAKING CHANGE: As the context extension is deprecated, this also removes the \"context\" item from results.</li> <li>Updated docs on automated updates of collection extents. (CLOSES #247)</li> <li>stac search now returns paging information using standards compliant links rather than prev/next properties (Fixes #265)</li> </ul>"},{"location":"release-notes/#fixed_6","title":"Fixed","text":"<ul> <li>Fixes issue when there is a None rather than an empty dictionary in hydration.</li> <li>Use \"debug\" log level rather than \"log\" to prevent growth in log messages due to differences in how client_min_messages and log_min_messages treat log levels. (Fixes #242)</li> <li>Refactor search_query and search_where functions to eliminate race condition when running identical queries. (Fixes #233)</li> <li>Fixes CQL2 Parser for Between operator (Fixes #251)</li> <li>Update PyO3 for rust hydration performance improvements.</li> </ul>"},{"location":"release-notes/#v086","title":"[v0.8.6]","text":""},{"location":"release-notes/#fixed_7","title":"Fixed","text":"<ul> <li>Relax version requirement for smart-open (Fixes #273)</li> <li>Use uv pip in docker build</li> </ul>"},{"location":"release-notes/#v085","title":"v0.8.5","text":""},{"location":"release-notes/#fixed_8","title":"Fixed","text":"<ul> <li>Fix issue when installing or migrating pgstac using a non superuser (particularly when using the default role found on RDS). (FIXES #239). Backports fix into migrations for 0.8.2, 0.8.3, and 0.8.4.</li> <li>Adds fixes/updates to documentation</li> <li>Fixes issue when using geometry with the strict queryables setting set.</li> </ul>"},{"location":"release-notes/#v084","title":"v0.8.4","text":""},{"location":"release-notes/#fixed_9","title":"Fixed","text":"<ul> <li>Make release deployment use postgres images without plrust</li> <li>Update versions of plrust in dockerfile (used for development, there is no plrust code yet)</li> <li>Update incremental migration tests to start at v0.3.0 rather than v0.1.9 due to a breaking change in pg_partman at version 5 that has no ability to pin a version. Migrating from prior to v0.3.0 should still work fine as long as pg_partman has not been updated on the database.</li> </ul>"},{"location":"release-notes/#v083","title":"v0.8.3","text":""},{"location":"release-notes/#added_3","title":"Added","text":"<ul> <li>Add support for arm64 to Docker images</li> </ul>"},{"location":"release-notes/#fixed_10","title":"Fixed","text":"<ul> <li>Fixes a critical bug when using the ingest_staging_upsert table or the upsert_item/upsert_items functions to update records with existing data where the existing row would get deleted, but the new row would not get added.</li> </ul>"},{"location":"release-notes/#v082","title":"v0.8.2","text":""},{"location":"release-notes/#added_4","title":"Added","text":"<ul> <li>Add support functions and tests for Collection Search</li> <li>Add configuration parameter for base_url to be able to generate absolute links</li> <li>With this release, this is only used to create links for paging in collection_search</li> <li>Adds read only mode to allow use of pgstac on read replicas</li> <li>Note: Turning on romode disables any caching (particularly when context is turned on) and does not allow to store q query hash that can be used with geometry_search.</li> <li>Add option to pypgstac loader \"--usequeue\" that forces use of the query queue for the loading process</li> <li>Add \"pypgstac runqueue\" command to run any commands that are set in the query queue</li> </ul>"},{"location":"release-notes/#fixed_11","title":"Fixed","text":"<ul> <li>Fix bug with end_datetime constraint management leading to inability to add data outside of constraints</li> <li>Fix bugs dealing with table ownership to ensure that all pgstac tables are owned by the pgstac_admin role</li> <li>Fixes issues with errors/warnings caused when doing index maintenance</li> <li>Fixes issues with errors/warnings caused with partition management</li> <li>Make sure that pgstac_ingest role always has read/write permissions on all tables</li> <li>Remove call to create_table_constraints from check_partition function. create_table_constraints was being called twice as it also gets called from update_partition_stats</li> <li>Add NOT NULL constraint to collections table (FIXES #224)</li> <li>Fix issue with indexes not getting created as the pg_admin role using SECURITY DEFINER</li> </ul>"},{"location":"release-notes/#changed_4","title":"Changed","text":"<ul> <li>Revert pydantic requirement back to '&gt;=1.7' and use basesettings conditionally from pydantic or pydantic.v1 to allow compatibility with pydantic 2 as well as with stac-fastapi that requires pydantic &lt;2</li> </ul>"},{"location":"release-notes/#v081","title":"v0.8.1","text":""},{"location":"release-notes/#fixed_12","title":"Fixed","text":"<ul> <li>Fix issue with CI building/pushing docker images</li> </ul>"},{"location":"release-notes/#v080","title":"v0.8.0","text":""},{"location":"release-notes/#fixed_13","title":"Fixed","text":"<ul> <li>Revert an optimisation which limited the number of results from a search query to the number of item IDs specified in the query. This fixes an issue where items with the same ID that are in multiple collections could be left out of search results.</li> </ul>"},{"location":"release-notes/#changed_5","title":"Changed","text":"<ul> <li>update <code>pydantic</code> requirement to <code>~=2.0</code></li> <li>update docker and ci workflows to build binary wheels for rust additions to pypgstac</li> <li>split docker into database service and python/rust container</li> <li>Modify scripts to auto-generate unreleased migration</li> <li>Add pre commit tasks to generate migration and to rebuild and compile pypgstac with maturin for rust</li> <li>Add private jsonb column to items and collections table to hold private metadata that should not be returned as part of a stac item</li> <li>Add generated columns to collections with the bounding box as a geometry and the datetime and end_datetime from the extents (this is to help with forthcoming work on collections search)</li> <li>Add PLRust to the Docker postgres image for forthcoming work to add optional PLRust functions for expensive json manipulation (including hydration)</li> <li>Remove default queryable for eo:cloud_cover</li> </ul>"},{"location":"release-notes/#v0710","title":"v0.7.10","text":""},{"location":"release-notes/#fixed_14","title":"Fixed","text":"<ul> <li>Return an empty jsonb array from all_collections() when the collections table is empty, instead of NULL. Fixes #186.</li> <li>Add delete trigger to collections to clean up partition_stats records and remove any partitions. Fixes #185</li> <li>Fixes boolean casting in get_setting_bool function</li> </ul>"},{"location":"release-notes/#v079","title":"v0.7.9","text":""},{"location":"release-notes/#fixed_15","title":"Fixed","text":"<ul> <li>Update docker image to use postgis 3.3.3</li> </ul>"},{"location":"release-notes/#v078","title":"v0.7.8","text":""},{"location":"release-notes/#fixed_16","title":"Fixed","text":"<ul> <li>Fix issue with search_query not returning all fields on first use of a query. Fixes #182</li> </ul>"},{"location":"release-notes/#v077","title":"v0.7.7","text":""},{"location":"release-notes/#fixed_17","title":"Fixed","text":"<ul> <li>Fix migrations for 0.7.4-&gt;0.7.5 and 0.7.5-&gt;0.7.6 to use the partition_view rather than the materialized view to avoid issue with refreshing the materialized view when run in the same statement that is accessing the view. Fixes #180.</li> </ul>"},{"location":"release-notes/#added_5","title":"Added","text":"<ul> <li>Add a short cirucit for id searches that sets the limit to be no more than the number of ids in the filter.</li> <li>Add 'timing' configuration variable that adds a \"timing\" element to the return object with the amount of time that it took to return a search.</li> <li>Reduce locking when updating statistics in the search table. Use skip locked to skip updating last_used and count when there is a lock being held.</li> </ul>"},{"location":"release-notes/#v076","title":"v0.7.6","text":""},{"location":"release-notes/#fixed_18","title":"Fixed","text":"<ul> <li>Fix issue with checking for existing collections in queryable trigger function that prevented adding scoped queryable entries.</li> </ul>"},{"location":"release-notes/#v075","title":"v0.7.5","text":""},{"location":"release-notes/#fixed_19","title":"Fixed","text":"<ul> <li>Default sort not getting set when sortby not included in query with token (Fixes #177)</li> <li>Fixes regression in performance between with changes for partition structure at v0.7.0. Changes the normal view for partitions and partition_steps into indexed materialized views. Adds refreshing of the views to existing triggers to make sure they stay up to date.</li> </ul>"},{"location":"release-notes/#v074","title":"v0.7.4","text":""},{"location":"release-notes/#added_6","title":"Added","text":"<ul> <li>Add --v and --vv options to scripts/test to change logging to notice / log when running tests.</li> <li>Add framework for option to cache expensive item formatting/hydrating calls. Note: this only provides functionality to add and read from the cached calls, but does not have any wiring to remove any entries from the cache.</li> <li>Update the costs for json formatting functions to 5000 to help the query planner choose to prefer using indexes on json fields.</li> </ul>"},{"location":"release-notes/#fixed_20","title":"Fixed","text":"<ul> <li>Fix bug in foreign key and unique collection detection in queryables trigger function, update tests to catch.</li> <li>Add collection id to tokens to ensure uniqueness and improve speed when looking up token values. Update tests to use the new keys. Old item id only tokens are still valid, but new results will all contain the new keys.</li> <li>Improve performance when looking for whether next/prev links should be added.</li> <li>Update Search function to remove the use of cursors and temp tables.</li> <li>Update get_token_filter to remove the use of temp tables.</li> </ul>"},{"location":"release-notes/#v073","title":"v0.7.3","text":""},{"location":"release-notes/#fixed_21","title":"Fixed","text":"<ul> <li>Use IF EXISTS when dropping constraints to avoid race conditions</li> <li>Rework function that finds indexes that need to be added to be added and to find functionally identical indexes better.</li> </ul>"},{"location":"release-notes/#v072","title":"v0.7.2","text":""},{"location":"release-notes/#fixed_22","title":"Fixed","text":"<ul> <li>Use version_parser for parsing versions in pypgstac</li> <li>Fix issue with dropping functions/procedures in 0.6.13-&gt;0.7.0 migrations</li> <li>Fix issue with CREATE OR REPLACE TRIGGER on PG 13</li> <li>Fix issue identifying duplicate indexes in maintain_partition_queries function</li> <li>Ensure that pgstac_read role has read permissions to all partitions</li> <li>Fix issue (and add tests) caused by bug in psycopg datetime types not being able to translate 'infinity', '-infinity'</li> </ul>"},{"location":"release-notes/#v071","title":"v0.7.1","text":""},{"location":"release-notes/#fixed_23","title":"Fixed","text":"<ul> <li>Fix permission issue when running incremental migrations.</li> <li>Make sure that pypgstac migrate runs in a single transaction</li> <li>Don't try to use concurrently when building indexes by default (this was tripping things up when using with pg_cron)</li> <li>Don't short circuit search for requests with ids (Fixes #159)</li> <li>Fix for issue with pagination when sorting by columns with nulls (Fixes #161 Fixes #152)</li> <li>Fixes issue where duplicate datetime,end_datetime index was being built.</li> <li>Fix bug in pypgstac loader when using delsert option</li> </ul>"},{"location":"release-notes/#added_7","title":"Added","text":"<ul> <li>Add trigger to detect duplicate configurations for name/collection combination in queryables</li> <li>Add trigger to ensure collections added to queryables exist</li> <li>Add tests for queryables triggers</li> <li>Add more tests for different pagination scenarios</li> </ul>"},{"location":"release-notes/#v070","title":"v0.7.0","text":""},{"location":"release-notes/#added_8","title":"Added","text":"<ul> <li>Reorganize code base to create clearer separation between pgstac sql code and pypgstac.</li> <li>Move Python tooling to use hatch with all python project configuration in pyproject.toml</li> <li>Rework testing framework to not rely on pypgstac or migrations. This allows to run tests on any code updates without creating a version first. If a new version has been staged, the tests will still run through all incremental migrations to make sure they pass as well.</li> <li>Add pre-commit to run formatting as well as the tests appropriate for which files have changed.</li> <li>Add a query queue to allow for deferred processing of steps that do not change the ability to get results, but enhance performance. The query queue allows to use pg_cron or similar to run tasks that are placed in the queue.</li> <li>Modify triggers to allow the use of the query queue for building indexes, adding constraints that are used solely for constraint exclusion, and updating partition and collection spatial and temporal extents. The use of the queue is controlled by the new configuration parameter \"use_queue\" which can be set as the pgstac.use_queue GUC or by setting in the pgstac_settings table.</li> <li>Reorganize how partitions are created and updated to maintain more metadata about partition extents and better tie the constraints to the actual temporal extent of a partition.</li> <li>Add \"partitions\" view that shows stats about number of records, the partition range, constraint ranges, actual date range and spatial extent of each partition.</li> <li>Add ability to automatically update the extent object on a collection using the partition metadata via triggers. This is controlled by the new configuration parameter \"update_collection_extent\" which can be set as the pgstac.update_collection_extent GUC or by setting in the pgstac_settings table. This can be combined with \"use_queue\" to defer the processing.</li> <li>Add many new tests.</li> <li>Migrations now make sure that all objects in the pgstac schema are owned by the pgstac_admin role. Functions marked as \"SECURITY DEFINER\" have been moved to the lower level functions responsible for creating/altering partitions and adding records to the search/search_wheres tables. This should open the door for approaches to using Row Level Security.</li> <li>Allow pypgstac loader to load data on pgstac databases that have the same major version even if minor version differs. [162] (stac-utils/pgstac#162) Cherry picked from stac-utils/pgstac!164.</li> </ul>"},{"location":"release-notes/#fixed_24","title":"Fixed","text":"<ul> <li>Allow empty strings in datetime intervals</li> <li>Set search_path and application_name upon connection rather than as kwargs for compatibility with RDS [156] (stac-utils/pgstac#156)</li> </ul>"},{"location":"release-notes/#v0613","title":"v0.6.13","text":""},{"location":"release-notes/#fixed_25","title":"Fixed","text":"<ul> <li>Fix issue with sorting and paging where in some circumstances the aggregation of data changed the expected order</li> </ul>"},{"location":"release-notes/#v0612","title":"v0.6.12","text":""},{"location":"release-notes/#added_9","title":"Added","text":"<ul> <li>Add ability to merge enum, min, and max from queryables where collections have different values.</li> <li>Add tooling in pypgstac and pgstac to add stac_extension definitions to the database.</li> <li>Modify missing_queryables function to try to use stac_extension definitions to populate queryable definitions from the stac_extension schemas.</li> <li>Add validate_constraints procedure</li> <li>Add analyze_items procedure</li> <li>Add check_pgstac_settings function to check system and pgstac settings.</li> </ul>"},{"location":"release-notes/#fixed_26","title":"Fixed","text":"<ul> <li>Fix issue with upserts in the trigger for using the items_staging tables</li> <li>Fix for generating token query for sorting. [152] (stac-utils/pgstac!152)</li> </ul>"},{"location":"release-notes/#v0611","title":"v0.6.11","text":""},{"location":"release-notes/#fixed_27","title":"Fixed","text":"<ul> <li>update pypgstac requirements to support python 3.11 142</li> <li>rename pgstac setting <code>default-filter-lang</code> to <code>default_filter_lang</code> to allow pgstac on postgresql&gt;=14</li> </ul>"},{"location":"release-notes/#v0610","title":"v0.6.10","text":""},{"location":"release-notes/#fixed_28","title":"Fixed","text":"<ul> <li>Makes sure that passing in a non-existing collection does not return a queryable object.</li> </ul>"},{"location":"release-notes/#v069","title":"v0.6.9","text":""},{"location":"release-notes/#fixed_29","title":"Fixed","text":"<ul> <li>Set cursor_tuple_fraction to 1 in search function to let query planner know to expect the entire table result within the search function to be returned. The default cursor_tuple_fraction of .1 within that function was at times creating bad query plans leading to slow queries.</li> </ul>"},{"location":"release-notes/#v068","title":"v0.6.8","text":""},{"location":"release-notes/#added_10","title":"Added","text":"<ul> <li>Add get_queryables function to return a composite queryables json for either a single collection (text), a list of collections(text[]), or for the full repository (null::text).</li> <li>Add missing_queryables(collection text, tablesample int) function to help identify if there are any properties in a collection without entries in the queryables table. The tablesample parameter is an int &lt;=100 that is the approximate percentage of the collection to scan to look for missing queryables rather than reading every item.</li> <li>Add missing_queryables(tablesample int) function that scans all collections using a sample of records to identify missing queryables.</li> </ul>"},{"location":"release-notes/#v067","title":"v0.6.7","text":""},{"location":"release-notes/#added_11","title":"Added","text":"<ul> <li>Add get_queryables function to return a composite queryables json for either a single collection (text), a list of collections(text[]), or for the full repository (null::text).</li> <li>Add missing_queryables(collection text, tablesample int) function to help identify if there are any properties in a collection without entries in the queryables table. The tablesample parameter is an int &lt;=100 that is the approximate percentage of the collection to scan to look for missing queryables rather than reading every item.</li> <li>Add missing_queryables(tablesample int) function that scans all collections using a sample of records to identify missing queryables.</li> </ul>"},{"location":"release-notes/#v066","title":"v0.6.6","text":""},{"location":"release-notes/#added_12","title":"Added","text":"<ul> <li>Add support for array operators in CQL2 (a_equals, a_contains, a_contained_by, a_overlaps).</li> <li>Add check in loader to make sure that pypgstac and pgstac versions match before loading data #123</li> </ul>"},{"location":"release-notes/#v065","title":"v0.6.5","text":""},{"location":"release-notes/#fixed_30","title":"Fixed","text":"<ul> <li>Fix for type casting when using the \"in\" operator #122</li> <li>Fix failure of pypgstac load for large items #121</li> </ul>"},{"location":"release-notes/#v064","title":"v0.6.4","text":""},{"location":"release-notes/#fixed_31","title":"Fixed","text":"<ul> <li>Fixed casts for numeric data when a property is not in the queryables table to use the type from the incoming json filter</li> <li>Fixed issue loader grouping an unordered iterable by partition, speeding up loads of items with mixed partitions #116</li> </ul>"},{"location":"release-notes/#v063","title":"v0.6.3","text":""},{"location":"release-notes/#fixed_32","title":"Fixed","text":"<ul> <li>Fixed content_hydrate argument ordering which caused incorrect behavior in database hydration #115</li> </ul>"},{"location":"release-notes/#added_13","title":"Added","text":"<ul> <li>Skip partition updates when unnecessary, which can drastically improve large ingest performance into existing partitions. #114</li> </ul>"},{"location":"release-notes/#v062","title":"v0.6.2","text":""},{"location":"release-notes/#fixed_33","title":"Fixed","text":"<ul> <li>Ensure special keys are not in content when loaded #112</li> </ul>"},{"location":"release-notes/#v061","title":"v0.6.1","text":""},{"location":"release-notes/#fixed_34","title":"Fixed","text":"<ul> <li>Fix issue where using equality operator against an array was only comparing the first element of the array</li> </ul>"},{"location":"release-notes/#v060","title":"v0.6.0","text":""},{"location":"release-notes/#fixed_35","title":"Fixed","text":"<ul> <li>Fix function signatures for transactional functions (delete_item etc) to make sure that they are marked as volatile</li> <li>Fix function for getting start/end dates from a stac item</li> </ul>"},{"location":"release-notes/#changed_6","title":"Changed","text":"<ul> <li>Update hydration/dehydration logic to make sure that it matches hydration/dehydration in pypgstac</li> <li>Update fields logic in pgstac to only use full paths and to match logic in stac-fastapi</li> <li>Always include id and collection on features regardless of fields setting</li> </ul>"},{"location":"release-notes/#added_14","title":"Added","text":"<ul> <li>Add tests to ensure that pgstac and pypgstac hydration logic is equivalent</li> <li>Add conf item to search to allow returning results without hydrating. This allows an application using pgstac to shift the CPU load of rehydrating items from the database onto the application server.</li> <li>Add \"--dehydrated\" option to loader to be able to load a dehydrated file (or iterable) of items such as would be output using pg_dump or postgresql copy.</li> <li>Add \"--chunksize\" option to loader that can split the processing of an iterable or file into chunks of n records at a time</li> </ul>"},{"location":"release-notes/#v051","title":"v0.5.1","text":""},{"location":"release-notes/#fixed_36","title":"Fixed","text":""},{"location":"release-notes/#changed_7","title":"Changed","text":""},{"location":"release-notes/#added_15","title":"Added","text":"<ul> <li>Add conf item to search to allow returning results without hydrating. This allows an application using pgstac to shift the CPU load of rehydrating items from the database onto the application server.</li> </ul>"},{"location":"release-notes/#v050","title":"v0.5.0","text":"<p>Version 0.5.0 is a major refactor of how data is stored. It is recommended to start a new database from scratch and to move data over rather than to use the inbuilt migration which will be very slow for larger amounts of data.</p>"},{"location":"release-notes/#fixed_37","title":"Fixed","text":""},{"location":"release-notes/#changed_8","title":"Changed","text":"<ul> <li> <p>The partition layout has been changed from being hardcoded to a partition to week to using nested partitions. The first level is by collection, for each collection, there is an attribute partition_trunc which can be set to NULL (no temporal partitions), month, or year.</p> </li> <li> <p>CQL1 and Query Code have been refactored to translate to CQL2 to reduce duplicated code in query parsing.</p> </li> <li> <p>Unused functions have been stripped from the project.</p> </li> <li> <p>Pypgstac has been changed to use Fire rather than Typo.</p> </li> <li> <p>Pypgstac has been changed to use Psycopg3 rather than Asyncpg to enable easier use as both sync and async.</p> </li> <li> <p>Indexing has been reworked to eliminate indexes that from logs were not being used. The global json index on properties has been removed. Indexes on individual properties can be added either globally or per collection using the new queryables table.</p> </li> <li> <p>Triggers for maintaining partitions have been updated to reduce lock contention and to reflect the new data layout.</p> </li> <li> <p>The data pager which optimizes \"order by datetime\" searches has been updated to get time periods from the new partition layout and partition metadata.</p> </li> <li> <p>Tests have been updated to reflect the many changes.</p> </li> </ul>"},{"location":"release-notes/#added_16","title":"Added","text":"<ul> <li>On ingest, the content in an item is compared to the metadata available at the collection level and duplicate information is stripped out (this is primarily data in the item_assets property). Logic is added in to merge this data back in on data usage.</li> </ul>"},{"location":"release-notes/#v045","title":"v0.4.5","text":""},{"location":"release-notes/#fixed_38","title":"Fixed","text":"<ul> <li>Fixes support for using the intersects parameter at the base of a search (regression from changes in 0.4.4)</li> <li>Fixes issue where results for a search on id returned [None] rather than [] (regression from changes in 0.4.4)</li> </ul>"},{"location":"release-notes/#changed_9","title":"Changed","text":"<ul> <li>Changes requirement for PostgreSQL to 13+, the triggers used to main partitions are not available to be used on partitions prior to 13 (#90)</li> <li>Bump requirement for asyncpg to 0.25.0 (#82)</li> </ul>"},{"location":"release-notes/#added_17","title":"Added","text":"<ul> <li>Added more tests.</li> </ul>"},{"location":"release-notes/#v044","title":"v0.4.4","text":""},{"location":"release-notes/#added_18","title":"Added","text":"<ul> <li>Adds support for using ids, collections, datetime, bbox, and intersects parameters separated from the filter-lang (Fixes #85)</li> <li>Previously use of these parameters was translated into cql-json and then to SQL, so was not available when using cql2-json</li> <li>The deprecated query parameter is still only available when filter-lang is set to cql-json</li> </ul>"},{"location":"release-notes/#changed_10","title":"Changed","text":"<ul> <li>Add PLPGSQL for item lookups by id so that the query plan for the simple query can be cached</li> <li>Use item_by_id function when looking up records used for paging filters</li> <li>Add a short circuit to search to use item_by_id lookup when using the ids parameter<ul> <li>This short circuit avoids using the query cache for this simple case</li> <li>Ordering when using the ids parameter is hard coded to return results in the same order as the array passed in (this avoids the overhead of full parsing and additional overhead to sort)</li> </ul> </li> </ul>"},{"location":"release-notes/#fixed_39","title":"Fixed","text":"<ul> <li>Fix to make sure that filtering on the search_wheres table leverages the functional index on the hash of the query rather than on the query itself.</li> </ul>"},{"location":"release-notes/#v043","title":"v0.4.3","text":""},{"location":"release-notes/#fixed_40","title":"Fixed","text":"<ul> <li>Fix for optimization when using equals with json properties. Allow optimization for both \"eq\" and \"=\" (was only previously enabled for \"eq\")</li> </ul>"},{"location":"release-notes/#v042","title":"v0.4.2","text":""},{"location":"release-notes/#changed_11","title":"Changed","text":"<ul> <li>Add support for updated CQL2 spec to use timestamp or interval key</li> </ul>"},{"location":"release-notes/#fixed_41","title":"Fixed","text":"<ul> <li>Fix for 0.3.4 -&gt; 0.3.5 migration making sure that partitions get renamed correctly</li> </ul>"},{"location":"release-notes/#v041","title":"v0.4.1","text":""},{"location":"release-notes/#changed_12","title":"Changed","text":"<ul> <li>Update <code>typer</code> to 0.4.0 to avoid clashes with <code>click</code> (#76)</li> </ul>"},{"location":"release-notes/#fixed_42","title":"Fixed","text":"<ul> <li>Fix logic in getting settings to make sure that filter-lang set on query is respected. (#77)</li> <li>Fix for large queries in the query cache. (#71)</li> </ul>"},{"location":"release-notes/#v040","title":"v0.4.0","text":""},{"location":"release-notes/#fixed_43","title":"Fixed","text":"<ul> <li>Fixes syntax for IN, BETWEEN, ISNULL, and NOT in CQL 1 (#69)</li> </ul>"},{"location":"release-notes/#added_19","title":"Added","text":"<ul> <li>Adds support for modifying settings through pgstac_settings table and by passing in 'conf' object in search json to support AWS RDS where custom user configuration settings are not allowed and changing settings on the fly for a given query.</li> <li>Adds support for CQL2-JSON (#67)</li> <li>Adds tests for all examples in github.com/radiantearth/stac-api-spec/blob/f5da775080ff3ff46d454c2888b6e796ee956faf/fragments/filter/README.md</li> <li>filter-lang parameter controls which dialect of CQL to use<ul> <li>Adds 'default-filter-lang' setting to control what dialect to use when 'filter-lang' is not present</li> <li>old style stac 'query' object and top level ids, collections, datetime, bbox, and intersects parameters are only available with cql-json</li> </ul> </li> </ul>"},{"location":"release-notes/#v034","title":"v0.3.4","text":""},{"location":"release-notes/#added_20","title":"Added","text":"<ul> <li>add <code>geometrysearch</code>, <code>geojsonsearch</code> and <code>xyzsearch</code> for optimized searches for tiled requets (#39)</li> <li>add <code>create_items</code> and <code>upsert_items</code> methods for bulk insert (#39)</li> </ul>"},{"location":"release-notes/#v033","title":"v0.3.3","text":""},{"location":"release-notes/#fixed_44","title":"Fixed","text":"<ul> <li>Fixed CQL term to be \"id\", not \"ids\" (#46)</li> <li>Make sure featureCollection response has empty features <code>[]</code> not <code>null</code> (#46)</li> <li>Fixed bugs for <code>sortby</code> and <code>pagination</code> (#46)</li> <li>Make sure pgtap errors get caught in CI (#46)</li> </ul>"},{"location":"release-notes/#v032","title":"v0.3.2","text":""},{"location":"release-notes/#fixed_45","title":"Fixed","text":"<ul> <li>Fixed CQL term to be \"collections\", not \"collection\" (#43)</li> </ul>"},{"location":"release-notes/#v031","title":"v0.3.1","text":"<p>TODO</p>"},{"location":"release-notes/#v028","title":"v0.2.8","text":""},{"location":"release-notes/#added_21","title":"Added","text":"<ul> <li>Type hints to pypgstac that pass mypy checks (#18)</li> </ul>"},{"location":"release-notes/#fixed_46","title":"Fixed","text":"<ul> <li>Fixed issue with pypgstac loads which caused some writes to fail (#18)</li> </ul>"}]}